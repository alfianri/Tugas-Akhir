\chapter{Perhitungan Manual Aktivasi \textit{Sigmoid, Softmax}, dan ReLU}

\subsection*{1. \textit{Sigmoid}}
Fungsi aktivasi \textit{sigmoid} mendefinisikan output sebagai:
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

Contoh: Misalkan \( x = 1.5 \).

Langkah-langkah:
\begin{enumerate}
    \item Hitung \( e^{-x} \):
    \[
    e^{-1.5} \approx 0.2231
    \]

    \item Tambahkan 1 ke hasil tersebut:
    \[
    1 + 0.2231 = 1.2231
    \]

    \item Hitung kebalikan (1 per hasil tersebut):
    \[
    \frac{1}{1.2231} \approx 0.8176
    \]
\end{enumerate}

Jadi, nilai aktivasi \textit{sigmoid} untuk \( x = 1.5 \) adalah sekitar \( 0.8176 \).

\subsection*{2. \textit{Softmax}}
Fungsi aktivasi \textit{softmax} menghitung probabilitas dari beberapa kelas. Untuk vektor input \(\mathbf{x} = [x_1, x_2, x_3, \ldots, x_n]\), fungsi \textit{softmax} mendefinisikan \textit{output} sebagai:
\[
\sigma(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\]

Contoh: Misalkan \(\mathbf{x} = [2.0, 1.0, 0.1]\).

Langkah-langkah:
\begin{enumerate}
    \item Hitung \( e^{x_i} \) untuk setiap elemen:
    \[
    e^{2.0} \approx 7.389
    \]
    \[
    e^{1.0} \approx 2.718
    \]
    \[
    e^{0.1} \approx 1.105
    \]

    \item Hitung jumlah semua hasil eksponensial:
    \[
    7.389 + 2.718 + 1.105 = 11.212
    \]

    \item Hitung setiap probabilitas dengan membagi hasil eksponensial masing-masing elemen dengan jumlah total:
    \[
    \sigma(x_1) = \frac{7.389}{11.212} \approx 0.659
    \]
    \[
    \sigma(x_2) = \frac{2.718}{11.212} \approx 0.242
    \]
    \[
    \sigma(x_3) = \frac{1.105}{11.212} \approx 0.099
    \]
\end{enumerate}

Jadi, hasil \textit{softmax} untuk \(\mathbf{x} = [2.0, 1.0, 0.1]\) adalah sekitar \([0.659, 0.242, 0.099]\).

\subsection*{3. ReLU (\textit{Rectified Linear Unit})}
Fungsi aktivasi ReLU mendefinisikan output sebagai:
\[
\text{ReLU}(x) = \max(0, x)
\]

Contoh: Misalkan \( x = -2.0, 1.0, 0.0, 3.5 \).

Langkah-langkah:
\begin{enumerate}
    \item Untuk \( x = -2.0 \):
    \[
    \text{ReLU}(-2.0) = \max(0, -2.0) = 0
    \]

    \item Untuk \( x = 1.0 \):
    \[
    \text{ReLU}(1.0) = \max(0, 1.0) = 1.0
    \]

    \item Untuk \( x = 0.0 \):
    \[
    \text{ReLU}(0.0) = \max(0, 0.0) = 0
    \]

    \item Untuk \( x = 3.5 \):
    \[
    \text{ReLU}(3.5) = \max(0, 3.5) = 3.5
    \]
\end{enumerate}

Jadi, hasil ReLU untuk \( x = [-2.0, 1.0, 0.0, 3.5] \) adalah \([0, 1.0, 0, 3.5]\).



\chapter{Perhitungan Manual Optimizer Adam dan SGD}

\subsection*{1. \textit{Stochastic Gradient Descent} (SGD)}

\textit{Stochastic Gradient Descent} (SGD) memperbarui parameter model berdasarkan perkiraan gradien dari fungsi biaya. Pembaruan dilakukan dengan langkah-langkah berikut:

\begin{enumerate}
    \item Pilih \textit{learning rate} \(\eta\).
    \item Hitung gradien dari fungsi biaya \(\nabla J(\theta)\) dengan \textit{respect to} parameter \(\theta\).
    \item Perbarui parameter \(\theta\) dengan langkah:
    \[
    \theta = \theta - \eta \nabla J(\theta)
    \]
\end{enumerate}

\textbf{Contoh:}

Misalkan kita memiliki fungsi biaya sederhana:
\[ 
J(\theta) = (\theta - 3)^2 
\]

Gradien dari fungsi biaya:
\[ 
\nabla J(\theta) = 2(\theta - 3) 
\]

Misalkan \(\theta = 0\) dan \(\eta = 0.1\).

\begin{enumerate}
    \item Hitung gradien:
    \[
    \nabla J(0) = 2(0 - 3) = -6
    \]

    \item Perbarui parameter \(\theta\):
    \[
    \theta = 0 - 0.1 \cdot (-6) = 0.6
    \]
\end{enumerate}

Iterasi selanjutnya (misalkan \(\theta = 0.6\)):

\begin{enumerate}
    \item Hitung gradien:
    \[
    \nabla J(0.6) = 2(0.6 - 3) = -4.8
    \]

    \item Perbarui parameter \(\theta\):
    \[
    \theta = 0.6 - 0.1 \cdot (-4.8) = 1.08
    \]
\end{enumerate}

Jadi, setelah dua iterasi, \(\theta\) diperbarui dari 0 menjadi 1.08.

\subsection*{2. Adam (\textit{Adaptive Moment Estimation})}

Adam adalah metode optimasi yang menggabungkan keuntungan dari dua metode, yaitu Adagrad dan RMSProp. Adam menggunakan estimasi momen pertama (\textit{mean}) dan kedua (\textit{variance}) dari gradien untuk melakukan pembaruan parameter. Langkah-langkah dalam Adam adalah sebagai berikut:

\begin{enumerate}
    \item Pilih \textit{learning rate} \(\eta\), \textit{decay rates} \(\beta_1\) dan \(\beta_2\), serta \textit{small constant} \(\epsilon\).
    \item Inialisasi \(m = 0\) dan \(v = 0\).
    \item Untuk setiap iterasi \(t\):
    \begin{itemize}
        \item Hitung gradien \(g_t\).
        \item Perbaharui momen pertama dan kedua:
        \[
        m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
        \]
        \[
        v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
        \]
        \item \textit{Bias correction}:
        \[
        \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
        \]
        \[
        \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
        \]
        \item Perbarui parameter:
        \[
        \theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
        \]
    \end{itemize}
\end{enumerate}

\textbf{Contoh:}

Misalkan kita memiliki fungsi biaya sederhana:
\[ 
J(\theta) = (\theta - 3)^2 
\]

Gradien dari fungsi biaya:
\[ 
\nabla J(\theta) = 2(\theta - 3) 
\]

Misalkan \(\theta = 0\), \(\eta = 0.1\), \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), \(\epsilon = 10^{-8}\).

Langkah-langkah untuk iterasi pertama:

\begin{enumerate}
    \item Hitung gradien:
    \[
    g_1 = 2(0 - 3) = -6
    \]

    \item Perbaharui momen pertama dan kedua:
    \[
    m_1 = 0.9 \cdot 0 + 0.1 \cdot (-6) = -0.6
    \]
    \[
    v_1 = 0.999 \cdot 0 + 0.001 \cdot (-6)^2 = 0.036
    \]

    \item \textit{Bias correction}:
    \[
    \hat{m}_1 = \frac{-0.6}{1 - 0.9} = -6
    \]
    \[
    \hat{v}_1 = \frac{0.036}{1 - 0.999} = 36
    \]

    \item Perbarui parameter:
    \[
    \theta_1 = 0 - 0.1 \cdot \frac{-6}{\sqrt{36} + 10^{-8}} \approx 0.1
    \]
\end{enumerate}

Iterasi selanjutnya (misalkan \(\theta = 0.1\)):

\begin{enumerate}
    \item Hitung gradien:
    \[
    g_2 = 2(0.1 - 3) = -5.8
    \]

    \item Perbaharui momen pertama dan kedua:
    \[
    m_2 = 0.9 \cdot (-0.6) + 0.1 \cdot (-5.8) = -1.14
    \]
    \[
    v_2 = 0.999 \cdot 0.036 + 0.001 \cdot (-5.8)^2 = 0.07164
    \]

    \item \textit{Bias correction}:
    \[
    \hat{m}_2 = \frac{-1.14}{1 - 0.9^2} = -6
    \]
    \[
    \hat{v}_2 = \frac{0.07164}{1 - 0.999^2} = 36
    \]

    \item Perbarui parameter:
    \[
    \theta_2 = 0.1 - 0.1 \cdot \frac{-6}{\sqrt{36} + 10^{-8}} \approx 0.2
    \]
\end{enumerate}

Jadi, setelah dua iterasi, \(\theta\) diperbarui dari 0 menjadi sekitar 0.2.