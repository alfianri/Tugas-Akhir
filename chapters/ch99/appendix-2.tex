\chapter{Perhitungan Manual Aktivasi \textit{Sigmoid, Softmax}, dan ReLU}

\subsection*{1. \textit{Sigmoid}}
Fungsi aktivasi \textit{sigmoid} mendefinisikan output sebagai:
\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

Contoh: Misalkan \( x = 1.5 \).

Langkah-langkah:
\begin{enumerate}
    \item Hitung \( e^{-x} \):
    \[
    e^{-1.5} \approx 0.2231
    \]

    \item Tambahkan 1 ke hasil tersebut:
    \[
    1 + 0.2231 = 1.2231
    \]

    \item Hitung kebalikan (1 per hasil tersebut):
    \[
    \frac{1}{1.2231} \approx 0.8176
    \]
\end{enumerate}

Jadi, nilai aktivasi \textit{sigmoid} untuk \( x = 1.5 \) adalah sekitar \( 0.8176 \).

\subsection*{2. \textit{Softmax}}
Fungsi aktivasi \textit{softmax} menghitung probabilitas dari beberapa kelas. Untuk vektor input \(\mathbf{x} = [x_1, x_2, x_3, \ldots, x_n]\), fungsi \textit{softmax} mendefinisikan \textit{output} sebagai:
\[
\sigma(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\]

Contoh: Misalkan \(\mathbf{x} = [2.0, 1.0, 0.1]\).

Langkah-langkah:
\begin{enumerate}
    \item Hitung \( e^{x_i} \) untuk setiap elemen:
    \[
    e^{2.0} \approx 7.389
    \]
    \[
    e^{1.0} \approx 2.718
    \]
    \[
    e^{0.1} \approx 1.105
    \]

    \item Hitung jumlah semua hasil eksponensial:
    \[
    7.389 + 2.718 + 1.105 = 11.212
    \]

    \item Hitung setiap probabilitas dengan membagi hasil eksponensial masing-masing elemen dengan jumlah total:
    \[
    \sigma(x_1) = \frac{7.389}{11.212} \approx 0.659
    \]
    \[
    \sigma(x_2) = \frac{2.718}{11.212} \approx 0.242
    \]
    \[
    \sigma(x_3) = \frac{1.105}{11.212} \approx 0.099
    \]
\end{enumerate}

Jadi, hasil \textit{softmax} untuk \(\mathbf{x} = [2.0, 1.0, 0.1]\) adalah sekitar \([0.659, 0.242, 0.099]\).

\subsection*{3. ReLU (\textit{Rectified Linear Unit})}
Fungsi aktivasi ReLU mendefinisikan output sebagai:
\[
\text{ReLU}(x) = \max(0, x)
\]

Contoh: Misalkan \( x = -2.0, 1.0, 0.0, 3.5 \).

Langkah-langkah:
\begin{enumerate}
    \item Untuk \( x = -2.0 \):
    \[
    \text{ReLU}(-2.0) = \max(0, -2.0) = 0
    \]

    \item Untuk \( x = 1.0 \):
    \[
    \text{ReLU}(1.0) = \max(0, 1.0) = 1.0
    \]

    \item Untuk \( x = 0.0 \):
    \[
    \text{ReLU}(0.0) = \max(0, 0.0) = 0
    \]

    \item Untuk \( x = 3.5 \):
    \[
    \text{ReLU}(3.5) = \max(0, 3.5) = 3.5
    \]
\end{enumerate}

Jadi, hasil ReLU untuk \( x = [-2.0, 1.0, 0.0, 3.5] \) adalah \([0, 1.0, 0, 3.5]\).



\chapter{Perhitungan Manual Optimizer Adam dan SGD}

\subsection*{1. \textit{Stochastic Gradient Descent} (SGD)}


\subsection*{Notasi dan Rumus}
\begin{itemize}
    \item \( W \): Parameter model yang akan diperbarui.
    \item \( \omega \): Nilai parameter \( W \) saat ini.
    \item \( \eta \): Learning rate.
    \item \( Q_i(\omega) \): Nilai fungsi \( Q_i \) untuk contoh data ke-i.
    \item \( Q(\omega) = \ln\left(\sum_i Q_i(\omega)\right) \).
    \item \( \nabla Q(\omega) = \ln\left(\sum_i \nabla Q_i(\omega)\right) \).
\end{itemize}

\subsection*{Rumus Pembaruan Parameter}
\begin{align}
    W & = \omega - \eta \cdot \nabla Q_i(\omega) \\
    Q(\omega) & = \ln\left(\sum_i Q_i(\omega)\right) \\
    \nabla Q(\omega) & = \ln\left(\sum_i \nabla Q_i(\omega)\right)
\end{align}

\subsection*{Contoh Perhitungan}
Misalkan kita memiliki dua contoh data (\( i = 1, 2 \)), dengan:
\begin{itemize}
    \item Nilai parameter saat ini: \( \omega = 1.0 \)
    \item Learning rate: \( \eta = 0.1 \)
    \item Fungsi \( Q_1(\omega) = \omega^2 \) dan \( Q_2(\omega) = 2\omega \)
\end{itemize}

Langkah-langkah perhitungan manual:

1. Hitung nilai \( Q_i(\omega) \) untuk \( \omega = 1.0 \):
   \begin{align*}
       Q_1(1.0) & = (1.0)^2 = 1.0 \\
       Q_2(1.0) & = 2 \cdot 1.0 = 2.0
   \end{align*}

2. Hitung nilai \( Q(\omega) \):
   \[
   Q(\omega) = \ln\left(Q_1(\omega) + Q_2(\omega)\right) = \ln(1.0 + 2.0) = \ln(3.0)
   \]

3. Hitung gradien \( \nabla Q_i(\omega) \):
   \begin{align*}
       \nabla Q_1(\omega) & = \frac{d}{d\omega}(\omega^2) = 2\omega \\
       \nabla Q_2(\omega) & = \frac{d}{d\omega}(2\omega) = 2
   \end{align*}
   Jadi, untuk \( \omega = 1.0 \):
   \begin{align*}
       \nabla Q_1(1.0) & = 2 \cdot 1.0 = 2 \\
       \nabla Q_2(1.0) & = 2
   \end{align*}

4. Hitung gradien \( \nabla Q(\omega) \):
   \[
   \nabla Q(\omega) = \ln\left(\nabla Q_1(\omega) + \nabla Q_2(\omega)\right) = \ln(2 + 2) = \ln(4)
   \]

5. Pembaruan parameter \( W \):
   \begin{align*}
       W & = \omega - \eta \cdot \nabla Q(\omega) \\
       & = 1.0 - 0.1 \cdot \ln(4)
   \end{align*}
   Menghitung nilai \( \ln(4) \approx 1.386 \):
   \[
   W = 1.0 - 0.1 \cdot 1.386 = 1.0 - 0.1386 = 0.8614
   \]

\subsection*{Kesimpulan}
Parameter model \( W \) setelah satu iterasi pembaruan menggunakan SGD adalah \( W \approx 0.8614 \).


\subsection*{2. Adam (\textit{Adaptive Moment Estimation})}

Adam adalah metode optimasi yang menggabungkan keuntungan dari dua metode, yaitu Adagrad dan RMSProp. Adam menggunakan estimasi momen pertama (\textit{mean}) dan kedua (\textit{variance}) dari gradien untuk melakukan pembaruan parameter. Langkah-langkah dalam Adam adalah sebagai berikut:

\begin{enumerate}
    \item Pilih \textit{learning rate} \(\eta\), \textit{decay rates} \(\beta_1\) dan \(\beta_2\), serta \textit{small constant} \(\epsilon\).
    \item Inialisasi \(m = 0\) dan \(v = 0\).
    \item Untuk setiap iterasi \(t\):
    \begin{itemize}
        \item Hitung gradien \(g_t\).
        \item Perbaharui momen pertama dan kedua:
        \[
        m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
        \]
        \[
        v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
        \]
        \item \textit{Bias correction}:
        \[
        \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
        \]
        \[
        \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
        \]
        \item Perbarui parameter:
        \[
        \theta_t = \theta_{t-1} - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
        \]
    \end{itemize}
\end{enumerate}

\textbf{Contoh:}

Misalkan kita memiliki fungsi biaya sederhana:
\[ 
J(\theta) = (\theta - 3)^2 
\]

Gradien dari fungsi biaya:
\[ 
\nabla J(\theta) = 2(\theta - 3) 
\]

Misalkan \(\theta = 0\), \(\eta = 0.1\), \(\beta_1 = 0.9\), \(\beta_2 = 0.999\), \(\epsilon = 10^{-8}\).

Langkah-langkah untuk iterasi pertama:

\begin{enumerate}
    \item Hitung gradien:
    \[
    g_1 = 2(0 - 3) = -6
    \]

    \item Perbaharui momen pertama dan kedua:
    \[
    m_1 = 0.9 \cdot 0 + 0.1 \cdot (-6) = -0.6
    \]
    \[
    v_1 = 0.999 \cdot 0 + 0.001 \cdot (-6)^2 = 0.036
    \]

    \item \textit{Bias correction}:
    \[
    \hat{m}_1 = \frac{-0.6}{1 - 0.9} = -6
    \]
    \[
    \hat{v}_1 = \frac{0.036}{1 - 0.999} = 36
    \]

    \item Perbarui parameter:
    \[
    \theta_1 = 0 - 0.1 \cdot \frac{-6}{\sqrt{36} + 10^{-8}} \approx 0.1
    \]
\end{enumerate}

Iterasi selanjutnya (misalkan \(\theta = 0.1\)):

\begin{enumerate}
    \item Hitung gradien:
    \[
    g_2 = 2(0.1 - 3) = -5.8
    \]

    \item Perbaharui momen pertama dan kedua:
    \[
    m_2 = 0.9 \cdot (-0.6) + 0.1 \cdot (-5.8) = -1.14
    \]
    \[
    v_2 = 0.999 \cdot 0.036 + 0.001 \cdot (-5.8)^2 = 0.07164
    \]

    \item \textit{Bias correction}:
    \[
    \hat{m}_2 = \frac{-1.14}{1 - 0.9^2} = -6
    \]
    \[
    \hat{v}_2 = \frac{0.07164}{1 - 0.999^2} = 36
    \]

    \item Perbarui parameter:
    \[
    \theta_2 = 0.1 - 0.1 \cdot \frac{-6}{\sqrt{36} + 10^{-8}} \approx 0.2
    \]
\end{enumerate}

Jadi, setelah dua iterasi, \(\theta\) diperbarui dari 0 menjadi sekitar 0.2.